# ü§ñ Rush Royale RL Bot - Next-Generation AI Gaming Assistant
<img width="1024" height="1024" alt="20250803_2330_RushBot App Logo_simple_compose_01k1rxd9atf21b3v5gkrpyxt0f" src="https://github.com/user-attachments/assets/621d866c-864e-42bb-a28a-c8dca66425a0" />

A cutting-edge **Python 3.13** reinforcement learning bot for "Rush Royale" that learns game mechanics from scratch through autonomous exploration. Built with hybrid AI architecture combining **Deep Q-Networks (DQN)** for strategic decisions and **Proximal Policy Optimization (PPO)** for real-time combat optimization.
This project is designed to push the boundaries of AI in gaming, enabling the bot to adapt, learn, and improve its performance over time without any prior knowledge of the game.

## Please note: This is a research project and not intended for commercial use. 
Most of the AI features are still under development and may not be fully functional. 
The Project Originally started as a project by AxelBjork (https://github.com/AxelBjork/Rush-Royale-Bot)
The Current main codebase is basically a fixed version of the original project with some improvements. Please check the [original project](https://github.com/AxelBjork/Rush-Royale-Bot) for more details on the current state.




[![Python](https://img.shields.io/badge/Python-3.13-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.3+-red.svg)](https://pytorch.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.16+-orange.svg)](https://tensorflow.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.9+-green.svg)](https://opencv.org/)
[![Stable-Baselines3](https://img.shields.io/badge/SB3-2.3+-purple.svg)](https://stable-baselines3.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **üöÄ Revolutionary Features**: Zero-knowledge learning ‚Ä¢ POI-based training ‚Ä¢ Hybrid RL architecture ‚Ä¢ Human-like behavior patterns ‚Ä¢ Self-improving strategies
This Bot is created in my Free Time, powered by the will to Learn something new. The Following Image shows the Future State in Developement.

<img width="3840" height="2291" alt="Untitled diagram _ Mermaid Chart-2025-08-03-205957" src="https://github.com/user-attachments/assets/ec46e243-9d00-401a-a54a-b7180c835d9c" />


---

## üéØ Revolutionary AI Features

### üß† Zero-Knowledge Reinforcement Learning
- **Tabula Rasa Learning**: Bot starts with ZERO game knowledge and learns everything through exploration
- **Points of Interest (POI) Training**: Systematic discovery of game elements (navigation, units, heroes, resources)
- **Curriculum Learning**: Progressive skill development from basic controls to advanced strategies
- **Self-Improving AI**: Continuous performance optimization through experience replay

### üéÆ Hybrid RL Architecture
- **DQN System**: Meta-game decisions (menu navigation, deck building, resource management)
- **PPO System**: Real-time combat optimization (unit placement, merging strategies, timing)
- **Multi-Agent Coordination**: Seamless integration between strategic and tactical AI layers
- **Adaptive Behavior**: Human-like decision patterns with built-in randomization

### ‚ö° Advanced Automation Capabilities
- **Intelligent Game State Recognition**: Dynamic adaptation to UI changes and game updates
- **Human-like Interaction Patterns**: Touch variance, timing randomization, fatigue simulation
- **Multi-Mode Operations**: Legacy automation + Hybrid RL + Full autonomous modes
- **Real-time Performance Monitoring**: Live metrics, learning curves, and optimization feedback

### üîÑ Legacy System Integration
- **Backward Compatibility**: Full support for existing Python 3.9 automation features
- **Gradual Migration**: Seamless transition from rule-based to RL-based decision making
- **Performance Benchmarking**: 15-25% efficiency improvement over legacy automation
- **Fallback Mechanisms**: Automatic fallback to proven strategies when RL exploration fails

---

## üèóÔ∏è Next-Generation AI Architecture

### üß† Reinforcement Learning Core

#### **DQN (Deep Q-Network) System** - Strategic Decision Making
```python
State Space: Menu states, available options, resource levels, progression metrics
Action Space: Navigation choices, purchasing decisions, mode selection, strategic planning
Reward Function: Progression milestones, resource optimization, efficiency metrics
Network: Input(84) ‚Üí Dense(512) ‚Üí Dense(256) ‚Üí Output(Action_Dim)
Training: Œµ-greedy exploration, Experience replay (50k), Target network updates (1000 steps)
```

#### **PPO (Proximal Policy Optimization) System** - Real-Time Combat
```python
State Space: 3x5 grid encoding, unit types/levels, mana, wave information, enemy patterns
Action Space: Placement coordinates (continuous), merge decisions (discrete), ability timing
Reward Function: Combat efficiency, survival time, resource utilization, strategic positioning
Network: Actor-Critic with shared features ‚Üí Policy(œÄ) + Value(V) heads
Training: Advantage estimation, Clipping Œµ=0.2, Batch size 2048, Learning rate 3e-4
```

### üìä POI (Points of Interest) Training System

#### **Phase 1: Foundation Discovery** (Weeks 1-2)
- **Navigation POIs**: Menu buttons, battle starts, settings, back navigation
- **Basic Recognition**: UI elements, clickable areas, state transitions
- **Error Recovery**: Handling unexpected states, connection issues, app crashes

#### **Phase 2: Game Mechanics Learning** (Weeks 3-6)
- **Unit POIs**: Card recognition, placement grid, merge mechanics, unit interactions
- **Hero POIs**: Hero selection, ability usage, upgrade paths, timing optimization
- **Resource POIs**: Mana management, gold/gem tracking, inventory optimization

#### **Phase 3: Advanced Strategy Development** (Weeks 7-12)
- **Combat Patterns**: Wave analysis, enemy prediction, defensive positioning
- **Meta-Game Optimization**: Deck building, progression planning, long-term strategy
- **Multi-Mode Mastery**: PvP tactics, Co-op coordination, Tournament strategies

### ÔøΩ Enhanced Computer Vision Pipeline

### üîß Enhanced Computer Vision Pipeline
- **Hybrid Detection**: Template matching for UI elements + Deep learning for game states
- **Multi-Threading**: 60+ FPS real-time analysis with parallel processing
- **Adaptive Recognition**: Self-adjusting thresholds based on game updates and lighting
- **State Extraction**: Advanced conversion of visual data to RL-compatible representations

### üì± Modernized Android Integration
- **Enhanced ADB Controller**: Built on existing scrcpy foundation with RL optimization
- **Intelligent Touch Patterns**: Human-like variance, gesture naturalness, anti-detection
- **Resolution Independence**: Dynamic coordinate mapping for multiple screen sizes
- **Robust Error Recovery**: Advanced handling of connection drops and state inconsistencies

---

## ‚öôÔ∏è Python 3.13 Migration & Setup

### üêç Modern Python Environment

**Why Python 3.13?**
- **15% Performance Boost**: Significantly faster execution for ML workloads
- **Enhanced Memory Management**: Improved garbage collection for long-running bots
- **Modern Features**: Pattern matching, improved asyncio, better type hints
- **ML Library Support**: Access to PyTorch 2.3+, TensorFlow 2.16+, latest Stable-Baselines3

```powershell
# Install Python 3.13 (Latest Release)
# https://www.python.org/downloads/

# Clone and setup repository
git clone https://github.com/mleem97/Rush-Royale-Bot.git
cd Rush-Royale-Bot

# Create Python 3.13 virtual environment
python3.13 -m venv .venv313
.venv313\Scripts\activate

# Install modern dependencies
pip install --upgrade pip
pip install -r requirements-py313.txt
```

**Enhanced Dependencies Stack:**
```
# Core ML & RL
torch>=2.3.0                 # PyTorch for neural networks
tensorflow>=2.16.0           # TensorFlow alternative backend
stable-baselines3>=2.3.0     # RL algorithms implementation
gymnasium>=0.29.0            # Modern OpenAI Gym replacement

# Computer Vision & Processing
opencv-python>=4.9.0         # Advanced image processing
numpy>=1.26.0                # Optimized numerical operations
pandas>=2.1.0                # Data manipulation with performance improvements

# Android & Communication
scrcpy-client>=0.2.0         # Low-latency device control
adb-shell>=0.4.4             # Enhanced ADB communication

# Utilities & Monitoring
tensorboard>=2.15.0          # Training visualization
wandb>=0.16.0                # Experiment tracking
tqdm>=4.66.0                 # Progress monitoring
```

### üì± Advanced Bluestacks Configuration

1. **Install Bluestacks 5** (Latest N-Beta for optimal performance)
2. **Performance Settings:**
   ```
   Display ‚Üí Resolution: 1600 x 900 (Required for POI recognition)
   Performance ‚Üí CPU: 4 cores, RAM: 8GB (for RL training stability)
   Graphics ‚Üí Engine: OpenGL (better for computer vision processing)
   Advanced ‚Üí ADB: Enabled on Port 5555
   ```
3. **Developer Options**: Enable USB debugging, Disable animations
4. **Network**: Stable connection for continuous learning sessions

---

## üéõÔ∏è Advanced Configuration System

### üìÑ `config.ini` - Enhanced Configuration
```ini
[rl_system]
# Reinforcement Learning Settings
enable_rl = True                                    # Enable RL system
rl_mode = hybrid                                    # legacy|hybrid|full_rl
training_phase = exploration                        # exploration|exploitation|self_play

[dqn_config]
# Deep Q-Network Configuration
learning_rate = 1e-4                               # DQN learning rate
epsilon_start = 1.0                                # Initial exploration rate
epsilon_end = 0.02                                 # Final exploration rate
epsilon_decay = 10000                              # Exploration decay steps
replay_buffer_size = 50000                        # Experience replay buffer
target_update_freq = 1000                         # Target network update frequency

[ppo_config] 
# Proximal Policy Optimization Configuration
learning_rate = 3e-4                              # PPO learning rate
batch_size = 2048                                 # Training batch size
n_steps = 2048                                    # Steps before update
clip_range = 0.2                                  # PPO clipping parameter
value_coef = 0.5                                  # Value function coefficient
entropy_coef = 0.01                               # Entropy bonus coefficient

[poi_training]
# Points of Interest Training Configuration
poi_categories = navigation,units,heroes,resources,combat
training_curriculum = foundation,mechanics,strategy
enable_curriculum_learning = True                  # Progressive difficulty
poi_discovery_bonus = 10.0                        # Reward for new POI discovery

[legacy_bot]
# Legacy System Configuration (Maintained for fallback)
floor = 10                                         # Target dungeon floor
mana_level = 1,3,5                                # Mana upgrade sequence
units = chemist,harlequin,bombardier,dryad,demon_hunter
dps_unit = demon_hunter                           # Primary damage dealer
pve = True                                        # PvE mode preference
require_shaman = False                            # Shaman opponent requirement
```

### üèóÔ∏è Multi-Mode Operation System
1. **Legacy Mode**: Original rule-based automation (100% backward compatible)
2. **Hybrid Mode**: RL-enhanced decisions with rule-based fallbacks
3. **Full RL Mode**: Pure reinforcement learning with minimal hard-coded logic
4. **Training Mode**: Dedicated environment for RL model development

---

## üöÄ Usage & Operation Modes

### üéÆ Production Mode (Enhanced GUI)
```powershell
launch_rl_gui.bat
```
**New Features:**
- **RL Training Monitor**: Live neural network performance metrics
- **POI Discovery Tracker**: Real-time learning progress visualization
- **Performance Comparisons**: Legacy vs RL efficiency metrics
- **Advanced Controls**: Training pause/resume, model checkpointing

### üß† RL Training Mode
```python
# train_bot.py - Main RL training script
from rl_core import RLTrainer
from poi_system import POIDiscovery

# Initialize RL training environment
trainer = RLTrainer(
    dqn_config='config/dqn_params.json',
    ppo_config='config/ppo_params.json',
    poi_system=POIDiscovery()
)

# Start curriculum learning
trainer.train_curriculum(
    phases=['foundation', 'mechanics', 'strategy'],
    total_timesteps=1_000_000,
    checkpoint_freq=10_000
)
```

### üî¨ Research & Development Mode
```python
# research_notebook.ipynb - Advanced experimentation
import torch
from stable_baselines3 import DQN, PPO
from rl_env import RushRoyaleEnv

# Custom environment for Rush Royale
env = RushRoyaleEnv(
    poi_system=True,
    curriculum_learning=True,
    human_like_behavior=True
)

# Experiment with different RL algorithms
models = {
    'DQN': DQN('MlpPolicy', env, learning_rate=1e-4),
    'PPO': PPO('MlpPolicy', env, learning_rate=3e-4),
    'Custom': CustomRLAgent(env)
}
```

---

## üéÆ Advanced RL Strategies & Learning

### üß† Curriculum Learning Progression

#### **Foundation Phase** (0-100k steps)
- **Basic Navigation**: Menu traversal, button recognition, state transitions
- **Simple Actions**: Click accuracy, swipe gestures, basic input validation
- **Error Recovery**: Handling crashes, reconnections, unexpected states
- **Success Metrics**: 95% navigation accuracy, <2% error rate

#### **Mechanics Phase** (100k-500k steps)  
- **Unit Management**: Placement strategies, merge optimization, mana efficiency
- **Combat Basics**: Wave survival, enemy pattern recognition, defensive positioning
- **Resource Optimization**: Gold/gem management, store interactions, upgrade decisions
- **Success Metrics**: Survive 20+ waves consistently, 80% merge efficiency

#### **Strategy Phase** (500k-1M+ steps)
- **Advanced Tactics**: Multi-unit synergies, timing optimization, strategic planning
- **Meta-Game Mastery**: Deck building, progression optimization, long-term planning
- **Adaptive Behavior**: Counter-strategies, opponent prediction, dynamic adaptation
- **Success Metrics**: Top 10% performance, human-level strategic decisions

### üéØ Reward Function Design

```python
def calculate_reward(self, state, action, next_state, info):
    reward = 0
    
    # Combat Performance Rewards
    reward += info['waves_survived'] * 10          # Survival bonus
    reward += info['enemies_killed'] * 1           # Combat efficiency
    reward += info['damage_dealt'] * 0.1           # Damage optimization
    
    # Resource Management Rewards  
    reward += info['mana_efficiency'] * 5          # Mana usage optimization
    reward += info['gold_gained'] * 0.01           # Economic efficiency
    reward += info['units_merged'] * 2             # Merge optimization
    
    # Strategic Decision Rewards
    reward += info['board_utilization'] * 3        # Space efficiency
    reward += info['unit_synergy_bonus'] * 15      # Strategic combinations
    reward += info['timing_bonus'] * 8             # Optimal action timing
    
    # POI Discovery Rewards
    reward += info['new_poi_discovered'] * 50      # Exploration bonus
    reward += info['poi_interaction_success'] * 5  # Learning progress
    
    # Penalty for failures
    reward -= info['action_failures'] * 10         # Invalid action penalty
    reward -= info['time_wasted'] * 2              # Efficiency penalty
    
    return reward
```

### üîÑ Self-Play & Advanced Training

- **Multi-Agent Training**: DQN vs PPO competitive learning
- **Adversarial Training**: Learning robust strategies against various opponents
- **Transfer Learning**: Knowledge transfer between game modes (PvE ‚Üî PvP)
- **Meta-Learning**: Quick adaptation to game updates and new mechanics

---

## üõ†Ô∏è Development & Research Tools

### üìä Advanced Analytics & Monitoring
```powershell
# Launch comprehensive monitoring dashboard
python tools/rl_dashboard.py --tensorboard --wandb --real-time

# Performance benchmarking suite
python tools/benchmark_rl.py --compare-all --export-results

# POI discovery analysis
python tools/poi_analyzer.py --visualize --export-heatmaps

# Model comparison utilities
python tools/model_comparison.py --legacy-vs-rl --statistical-analysis
```

### üß™ Research Experimentation Tools
- **Hyperparameter Optimization**: Automated tuning with Optuna/Ray Tune
- **Architecture Search**: Neural architecture optimization for game-specific tasks
- **Ablation Studies**: Component importance analysis and feature contribution
- **Robustness Testing**: Performance under various game conditions and updates

---

## üêõ Advanced Troubleshooting & Optimization

### ‚ùó RL-Specific Issues

#### Training Instability
```powershell
# Diagnose training issues
python tools/training_diagnostics.py --check-gradients --analyze-rewards

# Reset training with curriculum adjustment
python tools/reset_training.py --phase=mechanics --checkpoint=stable
```

#### Poor RL Performance
1. **Check Reward Function**: Ensure proper reward shaping and balance
2. **Verify POI Discovery**: Confirm adequate exploration of game elements
3. **Adjust Hyperparameters**: Use automated tuning for optimal settings
4. **Review Training Data**: Analyze experience replay buffer quality

#### Model Convergence Issues
- **Learning Rate Scheduling**: Implement adaptive learning rates
- **Network Architecture**: Experiment with different neural network designs
- **Training Stability**: Use gradient clipping and batch normalization
- **Curriculum Adjustment**: Modify training progression based on performance

### üìã Advanced Diagnostic Checklist
- [ ] Python 3.13 environment with latest ML libraries
- [ ] GPU acceleration available for neural network training
- [ ] Adequate compute resources (8GB+ RAM, 4+ CPU cores)
- [ ] Stable network connection for continuous learning
- [ ] Bluestacks optimized for RL training workloads
- [ ] Proper tensorboard/wandb logging configuration
- [ ] RL model checkpoints and backup systems functional

---

## üöÄ Performance & Success Metrics

### üìà Expected Improvements Over Legacy System
| Metric | Legacy Bot | RL Bot (Target) | Improvement |
|--------|-----------|----------------|-------------|
| **Dungeon Clear Rate** | 85% | 95%+ | +12% |
| **Resource Efficiency** | Baseline | +25% | +25% |
| **Adaptation Speed** | Manual Updates | Self-Learning | ‚àû |
| **Strategic Depth** | Rule-based | Human-level+ | Qualitative |
| **Multi-Mode Support** | Dungeon Only | All Game Modes | +400% |
| **Maintenance Overhead** | High | Minimal | -80% |

### üéØ Success Criteria & Milestones

#### **Phase 1 Success** (Month 1-2)
- [ ] Python 3.13 migration completed with 15%+ performance gain
- [ ] Basic POI system recognizing 50+ game elements
- [ ] DQN successfully navigating menus with 95%+ accuracy
- [ ] Hybrid mode operational with smooth legacy fallbacks

#### **Phase 2 Success** (Month 3-4)
- [ ] PPO combat system achieving 20+ wave survival consistency
- [ ] POI discovery covering 80%+ of game mechanics
- [ ] Self-play training generating diverse strategic behaviors
- [ ] Performance matching/exceeding legacy system efficiency

#### **Phase 3 Success** (Month 5-6)
- [ ] Full autonomous operation across all game modes
- [ ] Human-level strategic decision making
- [ ] Robust adaptation to game updates without retraining
- [ ] Research-grade documentation and reproducible results

---

## ü§ù Research Collaboration & Contributing

### üèóÔ∏è Development Guidelines for RL Systems
- **Reproducibility**: All experiments must be fully reproducible with seed control
- **Documentation**: Comprehensive docstrings and algorithm explanations
- **Testing**: Unit tests for RL components, integration tests for game interaction
- **Ethics**: Responsible AI development with fairness and transparency considerations

### üî¨ Research Contributions Welcome
- **Novel RL Algorithms**: Implementation of cutting-edge reinforcement learning methods
- **Game AI Insights**: Research into human-level gaming AI development
- **Transfer Learning**: Cross-game knowledge transfer mechanisms
- **Robustness Research**: Adversarial testing and failure mode analysis

### üìö Academic & Research Applications
This project serves as a comprehensive testbed for:
- **Multi-Agent Reinforcement Learning** research
- **Computer Vision in Gaming** applications  
- **Human-AI Interaction** studies
- **Curriculum Learning** methodology development
- **Game AI Strategy** optimization research

---

## üìú License & Research Ethics

This project is licensed under the **MIT License** with additional research ethics guidelines:

- **Educational Purpose**: Designed for learning and research in AI/ML
- **Responsible Use**: No commercial exploitation of game systems
- **Open Science**: Results and methodologies shared openly with community
- **Fair Play**: Bot detection avoidance through human-like behavior patterns

---

## üôè Acknowledgments & Research Community

### Core Technologies & Libraries
- **Stable-Baselines3** for state-of-the-art RL algorithm implementations
- **PyTorch/TensorFlow** for deep learning infrastructure
- **OpenCV** for advanced computer vision capabilities
- **OpenAI Gymnasium** for standardized RL environment interfaces

### Research Community
- **DeepMind** for pioneering work in game-playing AI
- **OpenAI** for foundational RL research and algorithm development  
- **RL Research Community** for continuous advancement of the field
- **Game AI Researchers** for insights into human-level gaming performance

---

## üìû Research Support & Community

- **GitHub Discussions**: Technical discussions and research collaboration
- **Research Papers**: Published methodologies and experimental results
- **Model Zoo**: Pre-trained models and checkpoints for research use
- **Benchmark Datasets**: Standardized evaluation datasets for comparison


